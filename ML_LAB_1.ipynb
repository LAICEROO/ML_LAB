{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrDs3nVFrwazqd3EKF7YVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LAICEROO/ML_LAB/blob/main/ML_LAB_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import potrzebnych bibliotek\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "VlZSgF9k8vXK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definicja funkcji aktywacji sigmoid(x) i jej pochodnej\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "def sigmoid_der(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))"
      ],
      "metadata": {
        "id": "_TlSEAUO8xlc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definicja klasy (wzoru/szablonu)\n",
        "#dla warstwy neuron√≥w z funkcja aktywacji sigmoid( wagi * wejscia + bias)\n",
        "#do latwego obliczania opdowiedzi warstwy neuronow na dane wejsciowe\n",
        "#oraz do aktualizacji wag tak aby zminimalizowac funkcje kosztu\n",
        "class Sigmoid:\n",
        "#tworzac obiekt reprezentujacy warstwe nalezy podac z ilu wejsc ma warstwa\n",
        "#odczytywac wartosci (zwykle jest to liczba wyjsc poprzedniej warstwy)\n",
        "#oraz liczbe neuronow w warstwie\n",
        "  def __init__(self, number_of_input_nodes, number_of_neurons):\n",
        "    self.weights = np.random.randn(number_of_input_nodes, number_of_neurons )\n",
        "    self.biases = np.random.randn(1,number_of_neurons)\n",
        "\n",
        "#obliczanie odpowiedzi warstwy na dane podane w input_data\n",
        "  def forward_prop(self, input_data):\n",
        "    self.orig_input = input_data\n",
        "    self.input_data = input_data #to be used in backprop\n",
        "    self.XW_B = np.dot(self.input_data, self.weights) + self.biases\n",
        "    self.output_val = sigmoid(self.XW_B)\n",
        "    return self.output_val\n",
        "\n",
        "#zmiana wag i biasow dla neuronow warstwy w kierunku powodujacym zmniejszenie\n",
        "#funkcji kosztu oraz obliczenie lancuszka pochodnych (gradientu) dla wczesniejszej warstwy\n",
        "#ktory jest potrzebny do analogicznej poprawy wartosci wag i biasow\n",
        "  def back_prop(self, gradient, learning_rate):\n",
        "\n",
        "    #pochodna funkcji aktywacji\n",
        "    layer_activation_der = sigmoid_der(self.XW_B)\n",
        "\n",
        "    #do obliczenia dJ_dweight dla warstwy wczesniejszej\n",
        "    #potrzebne sa aktualne wartosci wag aktualnej warstwy\n",
        "    dXWB__dinputs = self.weights.copy()\n",
        "\n",
        "    #obliczamy dJ_dwagi oraz DJ_dbiases dla aktualnej warstwy\n",
        "    dJ_dweights = self.input_data.T @ (gradient * layer_activation_der)\n",
        "    dJ_dbiases = np.sum((gradient * layer_activation_der),axis=0)\n",
        "\n",
        "    #zmieniamy wagi i biasy w kierunku ktory powoduje zmniejszenie funkcji kosztu J\n",
        "    #o czym mowi nam pochodna\n",
        "    self.weights -= learning_rate * dJ_dweights\n",
        "    self.biases -= learning_rate * dJ_dbiases\n",
        "\n",
        "    #obliczamy gradient_do_zwrotu (lancuch pochodnych) potrzebny\n",
        "    #do obliczenia dJ_dweights od dJ_biases przez wczesniejsza (po lewej) warstwe neuronow\n",
        "    gradient_do_zwrotu = (gradient * layer_activation_der) @ dXWB__dinputs.T\n",
        "    return gradient_do_zwrotu"
      ],
      "metadata": {
        "id": "4ax7CQdy81cs"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definicja ostatniej warstwy sieci neuronowej\n",
        "#do rozpoznania klasy obrazka \"wlozonego\" na wejscie sieci neuronowej\n",
        "#np pies lub kot\n",
        "#Warstwa ma tyle neuronow co klas do rozponania\n",
        "#Kazdy neuron zwraca prawdopodobienstwo ze obiekt na wejsciu nalezy\n",
        "#do klasy reprezentowanej przez niego\n",
        "\n",
        "class Softmax:\n",
        "  def __init__(self, input_node, softmax_node):\n",
        "    self.weight = np.random.randn(input_node, softmax_node)/input_node\n",
        "    self.bias = np.zeros(softmax_node)\n",
        "\n",
        "  def forward_prop(self, image):\n",
        "    self.orig_im_shape = image.shape #used in backprop\n",
        "    image_modified = image.flatten()\n",
        "    self.modified_input = image_modified #to be used in backprop\n",
        "    output_val = np.dot(image_modified, self.weight) + self.bias\n",
        "    self.out = output_val\n",
        "    exp_out = np.exp(output_val)\n",
        "    return exp_out/np.sum(exp_out, axis=0)\n",
        "\n",
        "  def back_prop(self, dL_dout, learning_rate):\n",
        "    for i, grad in enumerate(dL_dout):\n",
        "      if grad ==0:\n",
        "        continue\n",
        "\n",
        "      transformation_eq = np.exp(self.out)\n",
        "      S_total = np.sum(transformation_eq)\n",
        "\n",
        "      #gradients with respect to out (z)\n",
        "      dy_dz = -transformation_eq[i]*transformation_eq / (S_total **2)\n",
        "      dy_dz[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total **2)\n",
        "\n",
        "      #gradients of totals against weights/biases/input\n",
        "      dz_dw = self.modified_input\n",
        "      dz_db = 1\n",
        "      dz_d_inp = self.weight\n",
        "\n",
        "      #gradients of loss against totals\n",
        "      dL_dz = grad * dy_dz\n",
        "\n",
        "      #gradients of loss against weights/biases/input\n",
        "      dL_dw = dz_dw[np.newaxis].T @ dL_dz[np.newaxis]\n",
        "      dL_db = dL_dz * dz_db\n",
        "      dL_d_inp = dz_d_inp @ dL_dz\n",
        "\n",
        "    self.weight -= learning_rate *dL_dw\n",
        "    self.bias -= learning_rate * dL_db\n",
        "\n",
        "    return dL_d_inp.reshape(self.orig_im_shape)"
      ],
      "metadata": {
        "id": "j9-2OHjP9mWh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid_layer1 = Sigmoid(784,15)\n",
        "sigmoid_layer2 = Sigmoid(15,15)\n",
        "softmax = Softmax(15, 10)"
      ],
      "metadata": {
        "id": "xH7nQFGF-OLm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pobieramy zbior MNIST korzystajac z biblioteki keras\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "#bierzemy tylko 1500 pierwszych cyferek bo wolno sie liczy\n",
        "train_images = X_train[:1500]\n",
        "train_labels = y_train[:1500]\n",
        "test_images = X_test[:1500]\n",
        "test_labels = y_test[:1500]"
      ],
      "metadata": {
        "id": "0S8ufrjc-RIK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rozmiar tablicy przechowujacej cyferke\n",
        "print(train_images.shape)\n",
        "#rozmiar cyferki\n",
        "print(train_images[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn9Wy-_q-oVF",
        "outputId": "397513cd-c9df-4767-c483-2758a142a4d2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1500, 28, 28)\n",
            "(28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wyswietlamy LOSOWA cyferke z 1500 pierwszych\n",
        "from random import randint\n",
        "numer_obrazka=randint(0,1499)\n",
        "plt.imshow(train_images[numer_obrazka])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "vlt2rPct-pti",
        "outputId": "96aeb886-52f8-484e-c52f-603e4e25d66b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7dd2f8b599f0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splaszczenie obrazka\n",
        "img = train_images[numer_obrazka]\n",
        "img_flattened = np.reshape(img, (img.shape[0]*img.shape[1],1))\n",
        "print(img_flattened.shape)\n",
        "#fragment obrazka\n",
        "plt.imshow(img_flattened[200:215,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "3fgF1gbP-vXo",
        "outputId": "fb6aacc3-5f4b-4c52-aae0-d5c67201f044"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7dd304033ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFIAAAGdCAYAAACB5SROAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAARzklEQVR4nO3dfVBUZf/H8c/CsgsaLK7KAgVC5fiABBjCqI3iTyYzNW0mUweR0LEslBDHhJnAHDOyKaMHQ3NG0VKxP0Qd56eMww2poyAP5ZROPhQpaUBq7QqMqMt1/9G4dyugop+VA31fM/sHZ4/nOvv27BPsdVanlFIQD8ytq3egp5CQJBKSREKSSEgSCUkiIUkkJIm+q3fgdq2trbh48SK8vb2h0+lcMoZSClevXkVgYCDc3DjHkuZCXrx4EUFBQQ9lrNraWjz22GOUbWkupLe3NwDgGTwPPTxcMsZN3MBh/L9jLAbNhbx1d9bDA3qda0JCOY/FIE82JBKSREKSSEgSCUnispBr165FSEgIPD09ERsbi2PHjrlqKE1wScgdO3YgPT0dy5cvR3V1NSIiIjBhwgQ0NDS4YjhNcEnINWvWYP78+UhOTsbQoUOxbt069OrVCxs3bnTFcJpAD3n9+nVUVVUhPj7+f4O4uSE+Ph5Hjx5ts35LSwtsNpvTpTuih7x06RLsdjssFovTcovFgrq6ujbr5+TkwGQyOS4P6302W5c/a2dmZsJqtToutbW1Xb1L94X+Xrtfv35wd3dHfX290/L6+nr4+/u3Wd9oNMJoNLJ346GjH5EGgwFPP/00iouLHctaW1tRXFyMkSNHsofTDJf89ic9PR1JSUmIjo5GTEwMcnNz0dTUhOTkZFcMpwkuCTljxgz88ccfyM7ORl1dHSIjI7F///42T0A9iU5rn/2x2WwwmUyIw1SX/T7yprqBUuyG1WqFj48PZZtd/qzdU0hIEglJIiFJJCSJhCSRkCQSkkRCkkhIEglJIiFJJCSJhCSRkCQSkkRCkkhIEglJIiFJJCSJhCSRkCQSkkRCkkhIEglJIiFJJCSJhCSRkCQSkkRCkkhIEglJIiFJJCSJhCShh8zJycGIESPg7e0NPz8/TJs2DadOnWIPozn0kN9++y1SUlJQVlaGAwcO4MaNG3j22WfR1NTEHkpT6DO/9u/f7/Rzfn4+/Pz8UFVVhTFjxrCH0wyXn4nKarUCAMxmc7vXt7S0oKWlxfGzTHxvR2trK9LS0jB69GgMGzas3XVk4vs9SElJwY8//oiCgoIO15GJ73excOFC7N27FwcPHrzjqQV7ysR3ekilFBYtWoTCwkKUlpYiNDSUPYQm0UOmpKRg27Zt2L17N7y9vR0nBDGZTPDy8mIPpxn0x8i8vDxYrVbExcUhICDAcdmxYwd7KE1xyV3730jea5NISBIJSSIhSSQkiYQkkZAkEpJEQpJISBIJSSIhSSQkiYQkkZAkEpJEQpJISBIJSSIhSSQkiYQkkZAkEpJEQpJISBIJSSIhSSQkiYQkkZAkEpJEQpJISBIJSSIhSSQkiYQkcXnI999/HzqdDmlpaa4eqku5NGRFRQXWr1+Pp556ypXDaILLQjY2NiIhIQEbNmxAnz59XDWMZrgsZEpKCiZNmuT0ze89mUumGRcUFKC6uhoVFRV3XVfOINCB2tpavPnmm9i6dSs8PT3vun5POYMA/duMd+3ahRdffBHu7u6OZXa7HTqdDm5ubmhpaXG6rr0jMigoqNt9mzH9rj1+/Hj88MMPTsuSk5MxePBgLFu2zCkiIGcQ6JC3t3ebE4H07t0bffv27fAEIT2BvLMhcfl5fwCgtLT0YQzTpeSIJJGQJBKSREKSSEgSCUkiIUkkJImEJJGQJBKSREKSSEgSCUkiIUkkJImEJJGQJBKSREKSSEgSCUkiIUkkJImEJJGQJBKSREKSSEgSCUnyUD7Wdz/cennBTWdwzbbVdaCZvE3u5v69JCSJhCSRkCQSkkRCkkhIEglJ4pKQFy5cwOzZs9G3b194eXkhPDwclZWVrhhKM+jvbP7880+MHj0a48aNw759+9C/f3+cOXOmx09+p4dcvXo1goKCsGnTJseyf8M3GtPv2nv27EF0dDSmT58OPz8/REVFYcOGDR2u39LSApvN5nTpjughf/nlF+Tl5WHgwIEoKirC66+/jtTUVGzevLnd9WXiewcMBgOio6Nx5MgRx7LU1FRUVFTg6NGjbdbvaOL7//WaCb2LfvtzU13Hf5oLqBPf6UdkQEAAhg4d6rRsyJAhOH/+fLvrG41G+Pj4OF26I3rI0aNH49SpU07LTp8+jQEDBrCH0hR6yMWLF6OsrAzvvfcezp49i23btuHLL79ESkoKeyhNoYccMWIECgsLsX37dgwbNgwrV65Ebm4uEhIS2ENpikv+1DB58mRMnjzZFZvWLHmvTSIhSSQkiYQkkZAkmv2AQF3iU3A33v0kdffD3nINWF9A3aYckSQSkkRCkkhIEglJIiFJJCSJhCSRkCQSkkRCkkhIEglJIiFJJCSJhCSRkCQSkkRCkkhIEglJIiFJJCQJ/aPPD8pms8FkMuHbHx7FI96u+X9uvNqKseEXtP3R538rCUkiIUkkJImEJJGQJBKSREKS0EPa7XZkZWUhNDQUXl5eeOKJJ7By5Upo7HU/nUvma+fl5WHz5s0ICwtDZWUlkpOTYTKZkJqayh5OM+ghjxw5gqlTp2LSpEkAgJCQEGzfvh3Hjh1jD6Up9Lv2qFGjUFxcjNOnTwMAjh8/jsOHD2PixIntrt9TJr7Tj8iMjAzYbDYMHjwY7u7usNvtWLVqVYdzEXNycrBixQr2bjx09CPym2++wdatW7Ft2zZUV1dj8+bN+PDDDzs8g0BmZiasVqvjUltby96lh4J+RC5duhQZGRmYOXMmACA8PBznzp1DTk4OkpKS2qzfU77xnX5ENjc3w83NebPu7u5obW1lD6Up9CNyypQpWLVqFYKDgxEWFobvvvsOa9aswdy5c9lDaQo95GeffYasrCy88cYbaGhoQGBgIF577TVkZ2ezh9IU+VOD/KlBWyQkiYQkkZAkEpJEsxPfwwwe8DG45v/ZZuC/OZAjkkRCkkhIEglJIiFJJCSJhCSRkCQSkkRCkkhIEglJIiFJJCSJhCSRkCQSkkRCkkhIEglJIiFJJCSJhCSRkCQSkkRCkkhIEglJIiFJJCSJhCSRkCSdDnnw4EFMmTIFgYGB0Ol02LVrl9P1SilkZ2cjICAAXl5eiI+Px5kzZ1j7q1mdDtnU1ISIiAisXbu23es/+OADfPrpp1i3bh3Ky8vRu3dvTJgwAdeuXXvgndWyTn/0eeLEiR3OvVZKITc3F2+//TamTp0KANiyZQssFgt27drlmOjZE1EfI2tqalBXV4f4+HjHMpPJhNjY2Ha/WxvoORPfqSHr6uoAABaLxWm5xWJxXHe7nvKN713+rN1TJr5TQ/r7+wMA6uvrnZbX19c7rrudfON7O0JDQ+Hv74/i4mLHMpvNhvLycowcOZI5lOZ0+lm7sbERZ8+edfxcU1OD77//HmazGcHBwUhLS8O7776LgQMHIjQ0FFlZWQgMDMS0adOY+605nQ5ZWVmJcePGOX5OT08HACQlJSE/Px9vvfUWmpqa8Oqrr+Kvv/7CM888g/3798PT0zVfX6oVmp34fulUCHxcNPHddrUV/Qb9KhPftUhCkkhIEglJIiFJNDvx3V3nBneda/6f3XX8bcoRSSIhSSQkiYQkkZAkEpJEQpJISBIJSSIhSSQkiYQkkZAkEpJEQpJISBIJSSIhSSQkiYQkkZAkEpJEQpJISBLNfkDArlphd9EHDu1KzoyvWRKSREKSSEgSCUkiIUkkJAl14vuNGzewbNkyhIeHo3fv3ggMDMScOXNw8eJF5j5rEnXie3NzM6qrq5GVlYXq6mrs3LkTp06dwgsvvEDZWS2jTnw3mUw4cOCA07LPP/8cMTExOH/+PIKDg+9vL7sBlz9GWq1W6HQ6+Pr6unqoLuXS99rXrl3DsmXLMGvWrA6nqrW0tKClpcXxs5xB4DY3btzAyy+/DKUU8vLyOlxPziBwB7cinjt3DgcOHLjjxMmecgYB+l37VsQzZ86gpKQEffv2veP6RqMRRqORvRsPHXXie0BAAF566SVUV1dj7969sNvtjpOCmM1mGAwG3p5rTKfna5eWljpNfL8lKSkJ77zzDkJDQ9v9dyUlJYiLi7vr9rvrfO1OH5FxcXG4U3uNzaN/aOS9NomEJJGQJBKSREKSSEgSCUkiIUkkJImEJJGQJBKSREKSSEgSCUkiIUkkJImEJJGQJBKSREKSSEgSCUkiIUkkJImEJJGQJBKSREKSSEgSCUkiIUkkJImEJJGQJBKSREKSSEgSCUlCPYPA7RYsWACdTofc3NwH2MXugXoGgX8qLCxEWVkZAgMD73vnuhPqGQRuuXDhAhYtWoSioiJMmjTpvneuO6HPjm1tbUViYiKWLl2KsLCwu64vE987sHr1auj1eqSmpt7T+jLxvR1VVVX45JNPkJ+fD53u3r58sKdMfKeGPHToEBoaGhAcHAy9Xg+9Xo9z585hyZIlCAkJafffGI1G+Pj4OF26I+pjZGJiIuLj452WTZgwAYmJiUhOTmYOpTnUMwgEBwe3OfWCh4cH/P39MWjQoAffWw3rdMjKykqnMwikp6cD+PsMAvn5+bQd627oZxC43a+//trZIbolea9NIiFJJCSJhCTR3Andbz2RXW3kn3T9llvbZp5aR3MhL1++DAAIffr8QxnLZDJRtqW5kGazGQBw/vz5e76RNpsNQUFBqK2tvae3mFarFcHBwY6xGDQX0s3t74dtk8nU6ffdnX2vfmssBnmyIZGQJJoLaTQasXz58k6dU7Kz/+Z+xribTp/2ULRPc0dkdyUhSSQkiYQk0UTIK1euICEhAT4+PvD19cW8efPQ2NjotM7atWsREhICT09PxMbGYvjw4dDpdE6XBQsWONafN28ePDw8oNPp0KtXL3z88ccdjn/rr57/vHh6enbuRigNeO6551RERIQqKytThw4dUk8++aSaNWuW4/qCggJlMBjUxo0b1YkTJ9T8+fOVXq9Xs2fPVr///rvjYrValVJKrVixQgFQ06dPV3v27FFRUVEKgCotLW13/E2bNikfHx+nbdXV1XXqNnR5yJMnTyoAqqKiwrFs3759SqfTqQsXLiillIqJiVEpKSmO6+12uzIYDGrUqFHtbtNsNqsBAwY4re/h4aFiYmLaXX/Tpk3KZDI90O3o8rv20aNH4evri+joaMey+Ph4uLm5oby8HNevX0dVVZXTn3nd3NzQp08fVFZWol+/fhg2bBgyMzPR3NyM69ev48qVKxg/frzT+kOGDMFPP/3U4X40NjZiwIABCAoKwtSpU3HixIlO3Y4uD1lXVwc/Pz+nZXq9HmazGXV1dbh06RLsdjssFovTOhEREQgNDUVJSQkyMzPx1VdfYfbs2bh06RIAtPnKFz8/PzQ3N7e7D4MGDcLGjRuxe/dufP3112htbcWoUaPw22+/3fPtcFnIjIyMNg/gt1/udITcTUREBEwmE8LDw5GQkIAtW7agsLDwvv5qOXLkSMyZMweRkZEYO3Ysdu7cif79+2P9+vX3vA2X/RptyZIleOWVV+64zuOPPw5/f380NDQ4Lb958yauXLkCf39/9OvXD+7u7qivr3dap76+Hv7+/o6fY2NjAfz9CgD4+/eZ/9TQ0IBevXrd0757eHggKirK6YMQd/VAj7AEt55sKisrHcuKioraPNksXLjQcb3dblePPvqoysnJcSw7fPiwAqCOHz+uzGazCgkJcVrfYDB0+GRzu5s3b6pBgwapxYsX3/Pt6PKQSv398icqKkqVl5erw4cPq4EDBzq9/Pniiy+UTqdT2dnZ6uTJk2rmzJnK09NTFRUVqZqaGhUXF6d8fX3VmDFjlFL/e/kzY8YMtXfvXjV8+HAFQJWUlCillEpMTFQZGRmO7a9YsUIVFRWpn3/+WVVVVTm2f+LEiXu+DZoIefnyZTVr1iz1yCOPKB8fH5WcnKyuXr3quL6mpkYBUBaLRRkMBhUZGakiIyOV2WxWRqNReXp6qrCwMMfrSKWUmjt3rtLr9QqA8vLyUh999JHjurFjx6qkpCTHz2lpaSo4OFgZDAZlsVjU888/r6qrqzt1G+TXaCRd/vKnp5CQJBKSREKSSEgSCUkiIUkkJImEJJGQJBKSREKS/BcBHllWtxKEywAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#przelicz obrazek przez siec NN\n",
        "def forward_prop(image, label):\n",
        "  #splaszczenie\n",
        "  #print(\"rozmiary obrazka oryginalne\")\n",
        "  #print(image.shape)\n",
        "  image = np.reshape(image, (1, img.shape[0]*img.shape[1]))\n",
        "  #print(\"rozmiary obrazka przed wrzuceniem do sieci\")\n",
        "  #print(image.shape)\n",
        "\n",
        "  out_p = sigmoid_layer1.forward_prop((image /255) -0.5) #maly data engineering dla obrazka\n",
        "  out_p = sigmoid_layer2.forward_prop(out_p)\n",
        "  out_p = softmax.forward_prop(out_p)\n",
        "\n",
        "  #calculate cross-entropy loss and accuracy\n",
        "  cross_ent_loss = -np.log(out_p[label])\n",
        "\n",
        "  #sprawdzenie czy rozpoznano prawidlowo obrazek 1/TAK 0/NIE\n",
        "  accuracy_eval = 1 if np.argmax(out_p) == label else 0\n",
        "\n",
        "  return out_p, cross_ent_loss, accuracy_eval"
      ],
      "metadata": {
        "id": "v83pyACq-xc9"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test czy obrazek przejdzie przez siec\n",
        "shuffle_data = np.random.permutation(len(train_images))\n",
        "train_images = train_images[shuffle_data]\n",
        "train_labels = train_labels[shuffle_data]\n",
        "#jaka cyferka wrzucona na wejscie sieci\n",
        "plt.imshow(train_images[0])\n",
        "plt.show()\n",
        "#zaobaczmy co zwraca siec na podany na jej wejsciu obrazek\n",
        "wynik = forward_prop(train_images[0], train_labels[0])\n",
        "print(\"Wartosci na output neuronow warstwy softmax\")\n",
        "print(wynik[0])\n",
        "print(\"Wartosc funkcji kosztu\")\n",
        "print(wynik[1])\n",
        "print(\"Czy poprawnie sklasyfikowano 0 - NIE, 1 - TAK\")\n",
        "print(wynik[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "jmQqFNy6_HzO",
        "outputId": "7a384529-2712-4353-c8cb-ff38e8e2f819"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wartosci na output neuronow warstwy softmax\n",
            "[0.09594415 0.09344548 0.09008642 0.12823686 0.08385226 0.08597603\n",
            " 0.12724179 0.10722676 0.07961683 0.10837342]\n",
            "Wartosc funkcji kosztu\n",
            "2.061666166900581\n",
            "Czy poprawnie sklasyfikowano 0 - NIE, 1 - TAK\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#uczenie sieci\n",
        "def train_nn(image, label, learn_rate =0.005):\n",
        "  #forward\n",
        "  out, loss, acc = forward_prop(image, label)\n",
        "\n",
        "  #calculate ininital gradient\n",
        "  gradient = np.zeros(10)\n",
        "  gradient[label] = -1 / out[label]\n",
        "\n",
        "  #brackprop\n",
        "  grad_back = softmax.back_prop(gradient, learn_rate)\n",
        "  grad_back = sigmoid_layer2.back_prop( grad_back, learn_rate)\n",
        "  grad_back = sigmoid_layer1.back_prop( grad_back, learn_rate)\n",
        "\n",
        "  return loss, acc\n",
        "\n",
        "train_nn(train_images[0], train_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcvFHTWb_L-v",
        "outputId": "bfe236ed-9545-4de1-afb7-8a9f73ce1c67"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.061666166900581, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training cnn\n",
        "for epoch1 in range(14):\n",
        "  print('Epoch %d ->'% (epoch1 +1))\n",
        "\n",
        "  #shuffle the training data\n",
        "  shuffle_data = np.random.permutation(len(train_images))\n",
        "  train_images = train_images[shuffle_data]\n",
        "  train_labels = train_labels[shuffle_data]\n",
        "\n",
        "  #training the CNN\n",
        "  loss = 0.0\n",
        "  num_correct = 0\n",
        "\n",
        "  for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
        "    if i % 100 == 0:\n",
        "      print('%d steps out of 100 steps: Average Loss %.3f and Accuracy: %d%%' %(i+1, loss/100, num_correct))\n",
        "      loss = 0\n",
        "      num_correct = 0\n",
        "    #wrzucamy obrazek do sieci i poprawiamy wagi sieci aby\n",
        "    #siec w kolejnej epoce dla ten obrazek rozpoznala lepiej\n",
        "\n",
        "\n",
        "    l1, accu = train_nn(im, label)\n",
        "    loss += l1\n",
        "\n",
        "    num_correct +=accu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEM9-khgCQg8",
        "outputId": "a85e80db-bbaa-4403-a5c7-71a307706d24"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 2.133 and Accuracy: 30%\n",
            "201 steps out of 100 steps: Average Loss 2.106 and Accuracy: 35%\n",
            "301 steps out of 100 steps: Average Loss 2.175 and Accuracy: 24%\n",
            "401 steps out of 100 steps: Average Loss 2.044 and Accuracy: 35%\n",
            "501 steps out of 100 steps: Average Loss 2.120 and Accuracy: 32%\n",
            "601 steps out of 100 steps: Average Loss 2.104 and Accuracy: 31%\n",
            "701 steps out of 100 steps: Average Loss 2.140 and Accuracy: 28%\n",
            "801 steps out of 100 steps: Average Loss 2.142 and Accuracy: 24%\n",
            "901 steps out of 100 steps: Average Loss 2.181 and Accuracy: 24%\n",
            "1001 steps out of 100 steps: Average Loss 2.141 and Accuracy: 28%\n",
            "1101 steps out of 100 steps: Average Loss 2.100 and Accuracy: 26%\n",
            "1201 steps out of 100 steps: Average Loss 2.072 and Accuracy: 35%\n",
            "1301 steps out of 100 steps: Average Loss 2.074 and Accuracy: 38%\n",
            "1401 steps out of 100 steps: Average Loss 2.112 and Accuracy: 30%\n",
            "Epoch 2 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 2.080 and Accuracy: 34%\n",
            "201 steps out of 100 steps: Average Loss 2.090 and Accuracy: 34%\n",
            "301 steps out of 100 steps: Average Loss 2.107 and Accuracy: 31%\n",
            "401 steps out of 100 steps: Average Loss 2.125 and Accuracy: 25%\n",
            "501 steps out of 100 steps: Average Loss 2.122 and Accuracy: 21%\n",
            "601 steps out of 100 steps: Average Loss 2.054 and Accuracy: 40%\n",
            "701 steps out of 100 steps: Average Loss 2.081 and Accuracy: 28%\n",
            "801 steps out of 100 steps: Average Loss 2.075 and Accuracy: 30%\n",
            "901 steps out of 100 steps: Average Loss 2.049 and Accuracy: 34%\n",
            "1001 steps out of 100 steps: Average Loss 2.073 and Accuracy: 29%\n",
            "1101 steps out of 100 steps: Average Loss 1.988 and Accuracy: 42%\n",
            "1201 steps out of 100 steps: Average Loss 1.996 and Accuracy: 38%\n",
            "1301 steps out of 100 steps: Average Loss 2.143 and Accuracy: 25%\n",
            "1401 steps out of 100 steps: Average Loss 1.975 and Accuracy: 42%\n",
            "Epoch 3 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 2.030 and Accuracy: 30%\n",
            "201 steps out of 100 steps: Average Loss 2.005 and Accuracy: 36%\n",
            "301 steps out of 100 steps: Average Loss 2.040 and Accuracy: 33%\n",
            "401 steps out of 100 steps: Average Loss 1.980 and Accuracy: 38%\n",
            "501 steps out of 100 steps: Average Loss 2.081 and Accuracy: 23%\n",
            "601 steps out of 100 steps: Average Loss 1.942 and Accuracy: 45%\n",
            "701 steps out of 100 steps: Average Loss 1.947 and Accuracy: 42%\n",
            "801 steps out of 100 steps: Average Loss 2.040 and Accuracy: 29%\n",
            "901 steps out of 100 steps: Average Loss 2.047 and Accuracy: 35%\n",
            "1001 steps out of 100 steps: Average Loss 2.060 and Accuracy: 34%\n",
            "1101 steps out of 100 steps: Average Loss 2.036 and Accuracy: 32%\n",
            "1201 steps out of 100 steps: Average Loss 2.044 and Accuracy: 34%\n",
            "1301 steps out of 100 steps: Average Loss 1.955 and Accuracy: 40%\n",
            "1401 steps out of 100 steps: Average Loss 1.977 and Accuracy: 35%\n",
            "Epoch 4 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 2.073 and Accuracy: 30%\n",
            "201 steps out of 100 steps: Average Loss 1.917 and Accuracy: 39%\n",
            "301 steps out of 100 steps: Average Loss 1.938 and Accuracy: 41%\n",
            "401 steps out of 100 steps: Average Loss 1.966 and Accuracy: 36%\n",
            "501 steps out of 100 steps: Average Loss 1.927 and Accuracy: 46%\n",
            "601 steps out of 100 steps: Average Loss 1.961 and Accuracy: 39%\n",
            "701 steps out of 100 steps: Average Loss 1.924 and Accuracy: 42%\n",
            "801 steps out of 100 steps: Average Loss 1.959 and Accuracy: 38%\n",
            "901 steps out of 100 steps: Average Loss 1.934 and Accuracy: 41%\n",
            "1001 steps out of 100 steps: Average Loss 1.920 and Accuracy: 39%\n",
            "1101 steps out of 100 steps: Average Loss 1.922 and Accuracy: 42%\n",
            "1201 steps out of 100 steps: Average Loss 1.987 and Accuracy: 36%\n",
            "1301 steps out of 100 steps: Average Loss 1.893 and Accuracy: 41%\n",
            "1401 steps out of 100 steps: Average Loss 1.987 and Accuracy: 29%\n",
            "Epoch 5 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 1.880 and Accuracy: 44%\n",
            "201 steps out of 100 steps: Average Loss 1.884 and Accuracy: 47%\n",
            "301 steps out of 100 steps: Average Loss 1.929 and Accuracy: 36%\n",
            "401 steps out of 100 steps: Average Loss 1.883 and Accuracy: 45%\n",
            "501 steps out of 100 steps: Average Loss 1.832 and Accuracy: 39%\n",
            "601 steps out of 100 steps: Average Loss 1.904 and Accuracy: 38%\n",
            "701 steps out of 100 steps: Average Loss 1.849 and Accuracy: 43%\n",
            "801 steps out of 100 steps: Average Loss 1.842 and Accuracy: 42%\n",
            "901 steps out of 100 steps: Average Loss 1.825 and Accuracy: 42%\n",
            "1001 steps out of 100 steps: Average Loss 1.865 and Accuracy: 46%\n",
            "1101 steps out of 100 steps: Average Loss 1.917 and Accuracy: 39%\n",
            "1201 steps out of 100 steps: Average Loss 1.962 and Accuracy: 35%\n",
            "1301 steps out of 100 steps: Average Loss 1.908 and Accuracy: 38%\n",
            "1401 steps out of 100 steps: Average Loss 1.861 and Accuracy: 45%\n",
            "Epoch 6 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 1.880 and Accuracy: 40%\n",
            "201 steps out of 100 steps: Average Loss 1.863 and Accuracy: 41%\n",
            "301 steps out of 100 steps: Average Loss 1.807 and Accuracy: 44%\n",
            "401 steps out of 100 steps: Average Loss 1.648 and Accuracy: 56%\n",
            "501 steps out of 100 steps: Average Loss 1.856 and Accuracy: 39%\n",
            "601 steps out of 100 steps: Average Loss 1.822 and Accuracy: 40%\n",
            "701 steps out of 100 steps: Average Loss 1.838 and Accuracy: 39%\n",
            "801 steps out of 100 steps: Average Loss 1.852 and Accuracy: 41%\n",
            "901 steps out of 100 steps: Average Loss 1.867 and Accuracy: 45%\n",
            "1001 steps out of 100 steps: Average Loss 1.855 and Accuracy: 36%\n",
            "1101 steps out of 100 steps: Average Loss 1.752 and Accuracy: 51%\n",
            "1201 steps out of 100 steps: Average Loss 1.880 and Accuracy: 36%\n",
            "1301 steps out of 100 steps: Average Loss 1.711 and Accuracy: 51%\n",
            "1401 steps out of 100 steps: Average Loss 1.682 and Accuracy: 55%\n",
            "Epoch 7 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 1.713 and Accuracy: 51%\n",
            "201 steps out of 100 steps: Average Loss 1.842 and Accuracy: 39%\n",
            "301 steps out of 100 steps: Average Loss 1.830 and Accuracy: 40%\n",
            "401 steps out of 100 steps: Average Loss 1.799 and Accuracy: 46%\n",
            "501 steps out of 100 steps: Average Loss 1.795 and Accuracy: 46%\n",
            "601 steps out of 100 steps: Average Loss 1.673 and Accuracy: 54%\n",
            "701 steps out of 100 steps: Average Loss 1.733 and Accuracy: 45%\n",
            "801 steps out of 100 steps: Average Loss 1.720 and Accuracy: 51%\n",
            "901 steps out of 100 steps: Average Loss 1.711 and Accuracy: 51%\n",
            "1001 steps out of 100 steps: Average Loss 1.711 and Accuracy: 50%\n",
            "1101 steps out of 100 steps: Average Loss 1.708 and Accuracy: 45%\n",
            "1201 steps out of 100 steps: Average Loss 1.750 and Accuracy: 50%\n",
            "1301 steps out of 100 steps: Average Loss 1.745 and Accuracy: 45%\n",
            "1401 steps out of 100 steps: Average Loss 1.747 and Accuracy: 50%\n",
            "Epoch 8 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 1.674 and Accuracy: 52%\n",
            "201 steps out of 100 steps: Average Loss 1.739 and Accuracy: 47%\n",
            "301 steps out of 100 steps: Average Loss 1.761 and Accuracy: 47%\n",
            "401 steps out of 100 steps: Average Loss 1.693 and Accuracy: 50%\n",
            "501 steps out of 100 steps: Average Loss 1.672 and Accuracy: 51%\n",
            "601 steps out of 100 steps: Average Loss 1.658 and Accuracy: 53%\n",
            "701 steps out of 100 steps: Average Loss 1.587 and Accuracy: 54%\n",
            "801 steps out of 100 steps: Average Loss 1.647 and Accuracy: 52%\n",
            "901 steps out of 100 steps: Average Loss 1.689 and Accuracy: 47%\n",
            "1001 steps out of 100 steps: Average Loss 1.728 and Accuracy: 41%\n",
            "1101 steps out of 100 steps: Average Loss 1.558 and Accuracy: 59%\n",
            "1201 steps out of 100 steps: Average Loss 1.656 and Accuracy: 49%\n",
            "1301 steps out of 100 steps: Average Loss 1.595 and Accuracy: 50%\n",
            "1401 steps out of 100 steps: Average Loss 1.781 and Accuracy: 37%\n",
            "Epoch 9 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 1.531 and Accuracy: 59%\n",
            "201 steps out of 100 steps: Average Loss 1.665 and Accuracy: 48%\n",
            "301 steps out of 100 steps: Average Loss 1.630 and Accuracy: 51%\n",
            "401 steps out of 100 steps: Average Loss 1.613 and Accuracy: 47%\n",
            "501 steps out of 100 steps: Average Loss 1.529 and Accuracy: 56%\n",
            "601 steps out of 100 steps: Average Loss 1.688 and Accuracy: 43%\n",
            "701 steps out of 100 steps: Average Loss 1.504 and Accuracy: 52%\n",
            "801 steps out of 100 steps: Average Loss 1.600 and Accuracy: 49%\n",
            "901 steps out of 100 steps: Average Loss 1.592 and Accuracy: 52%\n",
            "1001 steps out of 100 steps: Average Loss 1.652 and Accuracy: 52%\n",
            "1101 steps out of 100 steps: Average Loss 1.522 and Accuracy: 56%\n",
            "1201 steps out of 100 steps: Average Loss 1.692 and Accuracy: 48%\n",
            "1301 steps out of 100 steps: Average Loss 1.628 and Accuracy: 56%\n",
            "1401 steps out of 100 steps: Average Loss 1.600 and Accuracy: 50%\n",
            "Epoch 10 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 1.554 and Accuracy: 55%\n",
            "201 steps out of 100 steps: Average Loss 1.626 and Accuracy: 51%\n",
            "301 steps out of 100 steps: Average Loss 1.538 and Accuracy: 57%\n",
            "401 steps out of 100 steps: Average Loss 1.580 and Accuracy: 59%\n",
            "501 steps out of 100 steps: Average Loss 1.497 and Accuracy: 59%\n",
            "601 steps out of 100 steps: Average Loss 1.619 and Accuracy: 52%\n",
            "701 steps out of 100 steps: Average Loss 1.656 and Accuracy: 47%\n",
            "801 steps out of 100 steps: Average Loss 1.457 and Accuracy: 60%\n",
            "901 steps out of 100 steps: Average Loss 1.592 and Accuracy: 50%\n",
            "1001 steps out of 100 steps: Average Loss 1.503 and Accuracy: 51%\n",
            "1101 steps out of 100 steps: Average Loss 1.516 and Accuracy: 53%\n",
            "1201 steps out of 100 steps: Average Loss 1.495 and Accuracy: 56%\n",
            "1301 steps out of 100 steps: Average Loss 1.454 and Accuracy: 54%\n",
            "1401 steps out of 100 steps: Average Loss 1.668 and Accuracy: 46%\n",
            "Epoch 11 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 1.487 and Accuracy: 53%\n",
            "201 steps out of 100 steps: Average Loss 1.608 and Accuracy: 49%\n",
            "301 steps out of 100 steps: Average Loss 1.442 and Accuracy: 56%\n",
            "401 steps out of 100 steps: Average Loss 1.455 and Accuracy: 55%\n",
            "501 steps out of 100 steps: Average Loss 1.611 and Accuracy: 50%\n",
            "601 steps out of 100 steps: Average Loss 1.440 and Accuracy: 53%\n",
            "701 steps out of 100 steps: Average Loss 1.574 and Accuracy: 49%\n",
            "801 steps out of 100 steps: Average Loss 1.437 and Accuracy: 65%\n",
            "901 steps out of 100 steps: Average Loss 1.383 and Accuracy: 67%\n",
            "1001 steps out of 100 steps: Average Loss 1.540 and Accuracy: 50%\n",
            "1101 steps out of 100 steps: Average Loss 1.554 and Accuracy: 49%\n",
            "1201 steps out of 100 steps: Average Loss 1.555 and Accuracy: 48%\n",
            "1301 steps out of 100 steps: Average Loss 1.483 and Accuracy: 54%\n",
            "1401 steps out of 100 steps: Average Loss 1.606 and Accuracy: 51%\n",
            "Epoch 12 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 1.507 and Accuracy: 50%\n",
            "201 steps out of 100 steps: Average Loss 1.575 and Accuracy: 45%\n",
            "301 steps out of 100 steps: Average Loss 1.363 and Accuracy: 56%\n",
            "401 steps out of 100 steps: Average Loss 1.583 and Accuracy: 54%\n",
            "501 steps out of 100 steps: Average Loss 1.586 and Accuracy: 46%\n",
            "601 steps out of 100 steps: Average Loss 1.429 and Accuracy: 56%\n",
            "701 steps out of 100 steps: Average Loss 1.436 and Accuracy: 60%\n",
            "801 steps out of 100 steps: Average Loss 1.477 and Accuracy: 51%\n",
            "901 steps out of 100 steps: Average Loss 1.444 and Accuracy: 56%\n",
            "1001 steps out of 100 steps: Average Loss 1.350 and Accuracy: 61%\n",
            "1101 steps out of 100 steps: Average Loss 1.446 and Accuracy: 52%\n",
            "1201 steps out of 100 steps: Average Loss 1.427 and Accuracy: 55%\n",
            "1301 steps out of 100 steps: Average Loss 1.311 and Accuracy: 67%\n",
            "1401 steps out of 100 steps: Average Loss 1.371 and Accuracy: 65%\n",
            "Epoch 13 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 1.537 and Accuracy: 52%\n",
            "201 steps out of 100 steps: Average Loss 1.402 and Accuracy: 52%\n",
            "301 steps out of 100 steps: Average Loss 1.401 and Accuracy: 56%\n",
            "401 steps out of 100 steps: Average Loss 1.321 and Accuracy: 55%\n",
            "501 steps out of 100 steps: Average Loss 1.423 and Accuracy: 58%\n",
            "601 steps out of 100 steps: Average Loss 1.392 and Accuracy: 57%\n",
            "701 steps out of 100 steps: Average Loss 1.385 and Accuracy: 54%\n",
            "801 steps out of 100 steps: Average Loss 1.559 and Accuracy: 55%\n",
            "901 steps out of 100 steps: Average Loss 1.389 and Accuracy: 56%\n",
            "1001 steps out of 100 steps: Average Loss 1.268 and Accuracy: 65%\n",
            "1101 steps out of 100 steps: Average Loss 1.399 and Accuracy: 54%\n",
            "1201 steps out of 100 steps: Average Loss 1.451 and Accuracy: 47%\n",
            "1301 steps out of 100 steps: Average Loss 1.455 and Accuracy: 53%\n",
            "1401 steps out of 100 steps: Average Loss 1.325 and Accuracy: 57%\n",
            "Epoch 14 ->\n",
            "1 steps out of 100 steps: Average Loss 0.000 and Accuracy: 0%\n",
            "101 steps out of 100 steps: Average Loss 1.447 and Accuracy: 52%\n",
            "201 steps out of 100 steps: Average Loss 1.302 and Accuracy: 59%\n",
            "301 steps out of 100 steps: Average Loss 1.487 and Accuracy: 52%\n",
            "401 steps out of 100 steps: Average Loss 1.293 and Accuracy: 53%\n",
            "501 steps out of 100 steps: Average Loss 1.281 and Accuracy: 64%\n",
            "601 steps out of 100 steps: Average Loss 1.413 and Accuracy: 55%\n",
            "701 steps out of 100 steps: Average Loss 1.419 and Accuracy: 54%\n",
            "801 steps out of 100 steps: Average Loss 1.389 and Accuracy: 58%\n",
            "901 steps out of 100 steps: Average Loss 1.423 and Accuracy: 55%\n",
            "1001 steps out of 100 steps: Average Loss 1.302 and Accuracy: 63%\n",
            "1101 steps out of 100 steps: Average Loss 1.316 and Accuracy: 57%\n",
            "1201 steps out of 100 steps: Average Loss 1.362 and Accuracy: 58%\n",
            "1301 steps out of 100 steps: Average Loss 1.389 and Accuracy: 58%\n",
            "1401 steps out of 100 steps: Average Loss 1.388 and Accuracy: 54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the accuracy of cnn\n",
        "\n",
        "print('**testing phase')\n",
        "loss = 0\n",
        "num_correct = 0\n",
        "\n",
        "for im, label in zip(test_images, test_labels):\n",
        "  _, l1, accu = forward_prop(im, label)\n",
        "  loss += l1\n",
        "  num_correct += accu\n",
        "\n",
        "num_test = len(test_images)\n",
        "print('Test Loss:', loss /num_test)\n",
        "print('Test accuracy:', num_correct / num_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZaGSfhNDuQE",
        "outputId": "c3a7b873-8200-4135-866c-7ee68639e07e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**testing phase\n",
            "Test Loss: 1.5648429453308836\n",
            "Test accuracy: 0.4786666666666667\n"
          ]
        }
      ]
    }
  ]
}